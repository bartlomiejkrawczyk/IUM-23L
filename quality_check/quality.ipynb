{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import views\n",
    "from spark import createSession\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_of_type(data_frame: DataFrame, type: str) -> List[str]:\n",
    "    return [column[0] for column in data_frame.dtypes if column[1] == type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH = 80\n",
    "def show_table_name(table: str) -> None:\n",
    "    print('=' * LENGTH)\n",
    "    print(' ' * ((LENGTH - len(table)) // 2), table.upper())\n",
    "    print('=' * LENGTH)\n",
    "\n",
    "def show_column_name(column: str) -> None:\n",
    "    print(column.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'v2'\n",
    "\n",
    "views = views(VERSION)\n",
    "spark = createSession()\n",
    "\n",
    "for view, file in views.items():\n",
    "    df = spark.read.json(file)\n",
    "    for column in get_columns_of_type(df, 'boolean'):\n",
    "        df = df.withColumn(column, F.col(column).cast(T.IntegerType()))\n",
    "    \n",
    "    for column in df.columns:\n",
    "        if column in ['timestamp', 'release_date']:\n",
    "            df = df.withColumn(f'{column}_s', F.unix_timestamp(column, \"yyyy[-MM-dd['T'HH:mm:ss[.SSSSSS]]]\"))\n",
    "\n",
    "    df.createOrReplaceTempView(view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FRAMES = list(zip(views.keys(), [spark.sql(f\"SELECT * FROM {view}\") for view in views.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for view, df in DATA_FRAMES:\n",
    "    show_table_name(view)\n",
    "    for column, type in df.dtypes:\n",
    "        print(column.upper(), '-', type)\n",
    "    \n",
    "    try:\n",
    "        dfp = df.limit(100_000).toPandas()\n",
    "        display(dfp)\n",
    "    except Exception as e:\n",
    "        df.show()\n",
    "        print(df.count(), 'rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for view, data_frame in DATA_FRAMES:\n",
    "    show_table_name(view)\n",
    "    for column, type in data_frame.dtypes:\n",
    "        show_column_name(column)\n",
    "        group_by_column = f\"\"\"--sql\n",
    "            SELECT \n",
    "                {column},\n",
    "                COUNT(*) AS length\n",
    "            FROM {view}\n",
    "            GROUP BY {column}\n",
    "            ORDER BY {column} IS NULL DESC, length DESC, {column} NULLS FIRST\n",
    "        \"\"\"\n",
    "        df = spark.sql(group_by_column)\n",
    "        display(df.limit(100_000).toPandas())\n",
    "\n",
    "        count_distinct = f\"\"\"--sql\n",
    "            SELECT\n",
    "                COUNT(DISTINCT {column})\n",
    "            FROM {view}\n",
    "        \"\"\"\n",
    "        df = spark.sql(count_distinct)\n",
    "        display(df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_numeric_column(view: str, column: str) -> str:\n",
    "    return f\"\"\"--sql\n",
    "            SELECT\n",
    "                \"{column}\" AS name,\n",
    "                COUNT({column}) AS count,\n",
    "                MIN({column}) AS min,\n",
    "                MAX({column}) AS max,\n",
    "                AVG({column}) AS average,\n",
    "                SUM({column}) AS sum,\n",
    "                SUM(DISTINCT {column}) AS sum_distinct,\n",
    "                KURTOSIS({column}) AS kurtosis,\n",
    "                SKEWNESS({column}) AS skewness,\n",
    "                STDDEV({column}) AS standard_deviation,\n",
    "                STDDEV_POP({column}) AS population_standard_deviation,\n",
    "                VARIANCE({column}) AS variance,\n",
    "                VAR_POP({column}) AS population_variance\n",
    "            FROM {view}\n",
    "            WHERE {column} IS NOT NULL\n",
    "        \"\"\"\n",
    "\n",
    "for view, data_frame in DATA_FRAMES:\n",
    "    show_table_name(view)\n",
    "    for column, type in data_frame.dtypes:\n",
    "        if type in ['double', 'bigint']:\n",
    "            show_column_name(column)\n",
    "            df = spark.sql(aggregate_numeric_column(view, column))\n",
    "            display(df.toPandas())\n",
    "\n",
    "            dfp = spark.sql(f\"SELECT {column} FROM {view}\").toPandas()\n",
    "            dfp.hist(bins=50)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_column(view: str, column: str) -> str:\n",
    "    return f\"\"\"--sql\n",
    "            SELECT\n",
    "                DISTINCT EXPLODE({column}) AS distinct_{column}\n",
    "            FROM {view}\n",
    "            ORDER BY distinct_{column} NULLS FIRST\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "def count_exploded_column(view: str, column: str) -> str:\n",
    "    exploded = f\"\"\"--sql\n",
    "        SELECT\n",
    "            DISTINCT EXPLODE({column}) AS {column}\n",
    "        FROM {view}\n",
    "    \"\"\"\n",
    "\n",
    "    return f\"\"\"--sql\n",
    "            SELECT\n",
    "                COUNT(*) AS length\n",
    "            FROM ({exploded})\n",
    "        \"\"\"\n",
    "\n",
    "for view, data_frame in DATA_FRAMES:\n",
    "    show_table_name(view)\n",
    "    for column, type in data_frame.dtypes:\n",
    "        if type.startswith('array'):\n",
    "            show_column_name(column)\n",
    "            df = spark.sql(explode_column(view, column))\n",
    "            display(df.toPandas())\n",
    "            df = spark.sql(count_exploded_column(view, column))\n",
    "            display(df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOINS = {\n",
    "    ('artists', 'tracks') : ('id', 'id_artist'),\n",
    "    ('tracks', 'track_storage') : ('id', 'track_id'),\n",
    "    ('tracks', 'sessions') : ('id', 'track_id'),\n",
    "    ('users', 'sessions') : ('user_id', 'user_id'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_everything(table: str) -> str:\n",
    "    return f\"\"\"--sql\n",
    "        SELECT\n",
    "            COUNT(*) AS length_{table}\n",
    "        FROM {table}\n",
    "    \"\"\"\n",
    "\n",
    "def count_joined(tables: Tuple[str, str], ids: Tuple[str, str]) -> str:\n",
    "    return f\"\"\"--sql\n",
    "        SELECT\n",
    "            COUNT(*) AS length_{tables[0]}_{tables[1]}\n",
    "        FROM {tables[0]} AS first\n",
    "        INNER JOIN {tables[1]} AS second ON first.{ids[0]} == second.{ids[1]}\n",
    "    \"\"\"\n",
    "\n",
    "def count_joined_distinct(tables: Tuple[str, str], ids: Tuple[str, str]) -> str:\n",
    "    return f\"\"\"--sql\n",
    "        SELECT\n",
    "            COUNT(DISTINCT first.{ids[0]}) AS length_{tables[0]}_{tables[1]}_distinct\n",
    "        FROM {tables[0]} AS first\n",
    "        INNER JOIN {tables[1]} AS second ON first.{ids[0]} == second.{ids[1]}\n",
    "    \"\"\"\n",
    "\n",
    "for tables, ids in JOINS.items():\n",
    "    print(tables[0].upper(), '-', tables[1].upper())\n",
    "    df = spark.sql(count_everything(tables[0]))\n",
    "    display(df.toPandas())\n",
    "    df = spark.sql(count_everything(tables[1]))\n",
    "    display(df.toPandas())\n",
    "    df = spark.sql(count_joined(tables, ids))\n",
    "    display(df.toPandas())\n",
    "    df = spark.sql(count_joined_distinct(tables, ids))\n",
    "    display(df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_unknown(tables: Tuple[str, str], ids: Tuple[str, str]) -> str:\n",
    "    spark.sql(f'SELECT DISTINCT {ids[1]} AS id FROM {tables[1]}') \\\n",
    "        .createOrReplaceTempView('temporary')\n",
    "\n",
    "    return f\"\"\"--sql\n",
    "        SELECT\n",
    "            *\n",
    "        FROM {tables[0]}\n",
    "        WHERE {ids[0]} NOT IN (SELECT id FROM temporary)\n",
    "    \"\"\"\n",
    "\n",
    "for tables, ids in JOINS.items():\n",
    "    print(tables[0].upper(), '-', tables[1].upper())\n",
    "    df = spark.sql(select_unknown(tables, ids))\n",
    "    display(df.toPandas())\n",
    "    df = spark.sql(select_unknown(tables[::-1], ids[::-1]))\n",
    "    display(df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premium_user_comparison = f\"\"\"--sql\n",
    "    SELECT\n",
    "        COUNT_IF(premium_user == 1) AS premium_users,\n",
    "        COUNT_IF(premium_user == 0) AS non_premium_users,\n",
    "        COUNT_IF(premium_user == 0) / COUNT(*) * 100 AS non_premium_users_percentage,\n",
    "        COUNT_IF(premium_user == 1) / COUNT(*) * 100 AS premium_users_percentage\n",
    "    FROM users\n",
    "\"\"\"\n",
    "df = spark.sql(premium_user_comparison)\n",
    "display(df.toPandas())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
