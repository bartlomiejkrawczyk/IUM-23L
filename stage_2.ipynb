{
   "cells": [
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Inżynieria Uczenia Maszynowego\n",
            "\n",
            "Studenci:\n",
            "```\n",
            "Bartłomiej Krawczyk\n",
            "Mateusz Brzozowski\n",
            "```"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Temat\n",
            "\n",
            "> “Jakiś czas temu wprowadziliśmy konta premium, które uwalniają użytkowników od słuchania reklam. Nie są one jednak jeszcze zbyt popularne – czy możemy się dowiedzieć, które osoby są bardziej skłonne do zakupu takiego konta?”"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import itertools\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import pickle\n",
            "import requests\n",
            "import seaborn as sns\n",
            "\n",
            "from IPython.display import display\n",
            "from matplotlib import pyplot as plt\n",
            "from math import sqrt\n",
            "from scipy.stats import uniform\n",
            "from sklearn.impute import SimpleImputer\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.dummy import DummyClassifier\n",
            "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
            "from sklearn.metrics import (\n",
            "    confusion_matrix,\n",
            "    roc_auc_score\n",
            ")\n",
            "from sklearn.pipeline import Pipeline\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from statistics import stdev, mean\n",
            "from typing import Any, Dict, Optional\n",
            "from xgboost import XGBClassifier\n",
            "\n",
            "from microservice import IUMModel\n",
            "from utility import Model"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Cechy i funkcje celu\n",
            "\n",
            "Do trenowania naszych modeli wykorzystaliśmy następujące cechy:\n",
            "- `number_of_advertisements`, ilość odtworzonych reklam w danym miesiącu\n",
            "- `number_of_tracks`, ilość przesłuchanych utworów w danym miesiącu\n",
            "- `number_of_skips`, ilość pominiętych utworów w danym miesiącu\n",
            "- `number_of_likes`, liczba danych lików w danym miesiącu\n",
            "- `number_of_liked_tracks_listened`, liczba przesłuchanych utworów w danym miesiącu, które w momencie odtworzenia były polubione\n",
            "- `number_of_tracks_in_favourite_genre`, liczba przesłuchanych utworów z ulubionego gatunku u danym miesiącu\n",
            "- `total_number_of_favourite_genres_listened`, liczba przesłuchanych gatunków w danym miesiącu należących do twoich ulubionych\n",
            "- `average_popularity_in_favourite_genres`, średnia popularność utworów wśród ulubionych gatunków w danym miesiącu\n",
            "- `total_tracks_duration_ms`, całkowity czas przesłuchanych utworów w danym miesiącu\n",
            "- `number_of_different_artists`, ilość przesłuchanych artystów w danym miesiącu\n",
            "- `average_release_date`, średnia data przesłuchanych piosenek w danym miesiącu\n",
            "- `average_duration_ms`, średni czas trwania utworów przesłuchanych w danym miesiącu\n",
            "- `explicit_tracks_ratio`, średnia ilość \"wulgarnych\" utworów przesłuchanych w danym miesiącu\n",
            "- `average_popularity`, średnia popularność przesłuchanych utworów w danym miesiącu\n",
            "- `average_acousticness`, średnia akustyka przesłuchanych utworów w danym miesiącu\n",
            "- `average_danceability`, średnia taneczność przesłuchanych utworów w danym miesiącu\n",
            "- `average_energy`, średnia moc przesłuchanych utworów w danym miesiącu\n",
            "- `average_instrumentalness`, średnia ilość utworów z wokalem przesłuchanych w danym miesiącu\n",
            "- `average_liveness`, średnie brzmienie utworów na żywo przesłuchanych w danym miesiącu\n",
            "- `average_loudness`, średnia głośność przesłuchanych utworów w danym miesiącu\n",
            "- `average_speechiness`, średnia ilość wokalu w utworach przesłuchanych w danym miesiącu\n",
            "- `average_tempo`, średnia prędkość przesłuchanych utworów w danym miesiącu\n",
            "- `average_valence`, średnia emocjonalność przesłuchanych utworów w danym miesiącu\n",
            "- `average_track_name_length`, średnia długość nazwy utworów przesłuchanych w danym miesiącu\n",
            "- `average_daily_cost`, średni koszt utrzymania przesłuchanych piosenek w danym miesiącu\n",
            "\n",
            "Posiadamy również dwie funkcje celu:\n",
            "- `premium_user_numerical`, gdzie będziemy przewidywać, czy użytkownik kiedykolwiek kupi premium\n",
            "- `will_buy_premium_next_month_numerical` przedstawiająca to czy użytkownik zakupi premium w przeciągu następnych 30 dni.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "FEATURE_VERSION = 'v1'\n",
            "FEATURE_PATH = f'features/{FEATURE_VERSION}/feature.csv'\n",
            "\n",
            "FEATURES = [\n",
            "    'number_of_advertisements',\n",
            "    'number_of_tracks',\n",
            "    'number_of_skips',\n",
            "    'number_of_likes',\n",
            "    'number_of_liked_tracks_listened',\n",
            "    'number_of_tracks_in_favourite_genre',\n",
            "    'total_number_of_favourite_genres_listened',\n",
            "    'average_popularity_in_favourite_genres',\n",
            "    'total_tracks_duration_ms',\n",
            "    'number_of_different_artists',\n",
            "    'average_release_date',\n",
            "    'average_duration_ms',\n",
            "    'explicit_tracks_ratio',\n",
            "    'average_popularity',\n",
            "    'average_acousticness',\n",
            "    'average_danceability',\n",
            "    'average_energy',\n",
            "    'average_instrumentalness',\n",
            "    'average_liveness',\n",
            "    'average_loudness',\n",
            "    'average_speechiness',\n",
            "    'average_tempo',\n",
            "    'average_valence',\n",
            "    'average_track_name_length',\n",
            "    'average_daily_cost',\n",
            "]\n",
            "\n",
            "TARGETS = [\n",
            "    'premium_user_numerical',\n",
            "    'will_buy_premium_next_month_numerical'\n",
            "]\n",
            "\n",
            "TARGET_AND_FEATURES = TARGETS + FEATURES"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data_frame = pd.read_csv(FEATURE_PATH)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Przykładowe wartości cech oraz funkcji celu"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data_frame.head()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Macierz korelacji cech z wartościami przewidywanymi\n",
            "Sprawdzamy korelację cech, które nie są zbytnio skorelowane między sobą, a za to są skorelowane z targetem. Na jej podstawie wybieramy wszystkie cechy, będą one używane do trenowania modeli."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "correlation_matrix = data_frame.loc[:, TARGET_AND_FEATURES] \\\n",
            "    .corr(method='spearman')\n",
            "\n",
            "plt.figure(figsize=(16, 16))\n",
            "\n",
            "sns.heatmap(\n",
            "    correlation_matrix,\n",
            "    xticklabels=correlation_matrix.columns,  # type: ignore\n",
            "    yticklabels=correlation_matrix.columns,  # type: ignore\n",
            "    annot=True,\n",
            "    annot_kws={\"fontsize\": 7},\n",
            "    fmt=\".0%\",\n",
            "    vmin=-1,\n",
            "    vmax=1,\n",
            ")\n",
            "\n",
            "plt.show()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Definiujemy pipeline do uzupełnienia danych pustych oraz przeskalowania danych"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pipeline = Pipeline([\n",
            "    (\"simple_imputer\", SimpleImputer()),\n",
            "    (\"standard_scaler\", StandardScaler())\n",
            "])"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Dzielimy dane na dane trenujące oraz testowe do późniejszych eksperymentów A/B"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "TRAINING_UP_TO = 2023\n",
            "TRAIN_DATA: pd.DataFrame = data_frame.loc[data_frame.year < TRAINING_UP_TO, :]\n",
            "TEST_DATA: pd.DataFrame = data_frame.loc[data_frame.year >= TRAINING_UP_TO, :]\n",
            "TEST_SIZE = 0.33"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Pipeline tworzony jest na podstawie, tylko i wyłącznie danych testowych\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "X_train_temp, X_test_temp, Y_train, Y_test = train_test_split(\n",
            "    TRAIN_DATA[FEATURES],\n",
            "    TRAIN_DATA[TARGETS],\n",
            "    test_size=TEST_SIZE\n",
            ")\n",
            "X_train_temp: pd.DataFrame\n",
            "X_test_temp: pd.DataFrame\n",
            "Y_train: pd.DataFrame\n",
            "Y_test: pd.DataFrame\n",
            "\n",
            "train_data = pipeline.fit_transform(X_train_temp)\n",
            "test_data = pipeline.transform(X_test_temp)\n",
            "X_train = pd.DataFrame(train_data, columns=FEATURES)\n",
            "X_test = pd.DataFrame(test_data, columns=FEATURES)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Cechy przetworzone przez pipeline"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "X_train.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "Y_train.head()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Modele\n",
            "\n",
            "Do porównywania wybraliśmy cztery modele:\n",
            "\n",
            "- `Dummy` - naiwny model, który zawsze przewiduje najczęściej występującą klasę\n",
            "- `Logistic Regression` - model regresji logistycznej z domyślnymi parametrami\n",
            "- `XGB Classifier` - model XGBoost z domyślnymi parametrami\n",
            "- `XGB Classifier with Randomize Search` - model XGBoost z Randomize Search. Randomize Search to metoda optymalizacji hiperparametrów, która losowo wybiera zdefiniowaną liczbę kombinacji hiperparametrów i zwraca najlepszą z nich. W ten sposób można znaleźć dobre parametry modelu bez konieczności przeszukiwania całej przestrzeni hiperparametrów."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "DUMMY = 'dummy'\n",
            "LOGISTIC_REG = 'logistic_regression'\n",
            "XGB = 'xgb_classifier'\n",
            "XGB_BEST_ESTIMATOR = 'xgb_classifier_best_estimator'\n",
            "RANDOM = 'randomized_search'\n",
            "\n",
            "\n",
            "MODEL_TYPES = [DUMMY, LOGISTIC_REG, XGB, XGB_BEST_ESTIMATOR]\n",
            "\n",
            "\n",
            "def construct_dummy(X_train: pd.DataFrame, y_train: pd.DataFrame, params: Optional[Dict[str, Any]] = None) -> DummyClassifier:\n",
            "    return DummyClassifier().fit(X_train, y_train)\n",
            "\n",
            "\n",
            "def construct_logistic_reggression(X_train: pd.DataFrame, y_train: pd.DataFrame, params: Optional[Dict[str, Any]] = None) -> LogisticRegression:\n",
            "    return LogisticRegression().fit(X_train, y_train)\n",
            "\n",
            "\n",
            "def construct_xgb_classifier(X_train: pd.DataFrame, y_train: pd.DataFrame, params: Optional[Dict[str, Any]] = None) -> XGBClassifier:\n",
            "    return XGBClassifier().fit(X_train, y_train)\n",
            "\n",
            "\n",
            "def construct_xgb_classifier_with_randomized_search(X_train: pd.DataFrame, y_train: pd.DataFrame, params: Optional[Dict[str, Any]] = None) -> XGBClassifier:\n",
            "    scale = y_train.value_counts()\n",
            "    if params:\n",
            "        return XGBClassifier(**params).fit(X_train, y_train)\n",
            "    model = XGBClassifier(scale_pos_weight=sqrt(scale[0] / scale[1]))\n",
            "    # TODO: update with own parameters\n",
            "\n",
            "    randomized_search_cv = RandomizedSearchCV(\n",
            "        estimator=model,\n",
            "        param_distributions={\n",
            "            'max_depth': range(3, 33),  # range(3, 30),\n",
            "            'eta': uniform(0, 0.25),\n",
            "            'gamma': uniform(0, 1),\n",
            "            'n_estimators': range(10, 100),\n",
            "        },\n",
            "        n_iter=30,  # 20,\n",
            "        scoring='roc_auc',\n",
            "        n_jobs=-1,\n",
            "        verbose=3,\n",
            "    )\n",
            "    estimator = randomized_search_cv.fit(X_train, y_train)\n",
            "    return estimator.best_estimator_  # type: ignore\n",
            "\n",
            "\n",
            "MODEL_CONSTRUCTORS = {\n",
            "    DUMMY: construct_dummy,\n",
            "    LOGISTIC_REG: construct_logistic_reggression,\n",
            "    XGB: construct_xgb_classifier,\n",
            "    XGB_BEST_ESTIMATOR: construct_xgb_classifier_with_randomized_search\n",
            "}\n",
            "MODELS: Dict[str, Dict[str, Model]] = {}\n",
            "\n",
            "for type in MODEL_TYPES:\n",
            "    MODELS[type] = {\n",
            "        target: MODEL_CONSTRUCTORS[type](X_train, Y_train[target])\n",
            "        for target in TARGETS\n",
            "    }"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Ocena modeli\n",
            "\n",
            "Posiadamy niezbilansowane dane, dlatego też do oceny modeli wykorzystaliśmy metrykę `ROC-AUC`, która jest miarą jakości klasyfikatora binarnego. \n",
            "\n",
            "`ROC-AUC` mierzy zdolność modelu do rozróżnienia między dwiema klasami poprzez obliczenie powierzchni pod krzywą ROC. Krzywa ROC przedstawia zależność między wskaźnikiem True Positive Rate = TP / ( TP + FN) (czułość) a False Positive Rate = FP / (FP + TN) (specyficzność). Wyższa wartość ROC-AUC oznacza lepszą zdolność modelu do rozróżniania klas.\n",
            "\n",
            "Nie wykorzystaliśmy metryki `accuracy`, ponieważ w przypadku niezbilansowanych danych, może ona być myląca. Przykładowo, jeśli mamy 1000 obserwacji, z czego 900 należy do klasy 0, a 100 do klasy 1, to model, który zawsze zwraca 0, będzie miał accuracy 90%."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for type in MODEL_TYPES:\n",
            "    print(type.upper())\n",
            "    _, axs = plt.subplots(1, 2, figsize=(24, 10))  # type: ignore\n",
            "    for i, target in enumerate(TARGETS):\n",
            "        model = MODELS[type][target]\n",
            "        y_predicted = model.predict(X_test)\n",
            "        y_true = Y_test[target]\n",
            "        roc_auc_score_value = roc_auc_score(y_true, y_predicted)\n",
            "        print(f\"ROC AUC score for {target}: {roc_auc_score_value}\")\n",
            "        matrix = confusion_matrix(y_true, y_predicted)\n",
            "        sns.heatmap(\n",
            "            matrix,\n",
            "            annot=True,\n",
            "            annot_kws={\"fontsize\": 30},\n",
            "            fmt='g',\n",
            "            xticklabels=[\"0\", \"1\"],  # type: ignore\n",
            "            yticklabels=[\"0\", \"1\"],  # type: ignore\n",
            "            ax=axs[i]  # type: ignore\n",
            "        )\n",
            "    plt.show()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Istotność parametrów\n",
            "Analizując wyniki możemy zauważyć, że dla przewidywania `premium_user_numerical` (czy użytkownik kiedykolwiek zakupi premium) najgorzej poradził sobie model naiwny `Dummy`, który każdemu przypisuje klasę większościową. Nieznacznie lepsze wyniki na podobnym poziomie, osiągnęły modele `Logistic Regression` oraz `XGB Classifier`. Najlepsze wyniki osiągnął model `XGB Classifier with Randomize Search`, dzięki optymalizacji hiperparametrów. W Przypadku `will_buy_premium_next_month_numerical` wyniki dla `Dummy`, `Logistic Regression` oraz `XGB Classifier` są bliskie 0.5, tak więc model nie jest w stanie przewidzieć czy użytkownik kupi premium w następnym miesiącu. Jedynie model `XGB Classifier with Randomize Search` osiągnął wynik 0.53, co jest nieznacznie lepsze od losowego przewidywania."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def retrieve_weights(model: Model) -> np.ndarray[np.float64]:\n",
            "    if isinstance(model, LogisticRegression):\n",
            "        return model.coef_[0]\n",
            "    if isinstance(model, XGBClassifier):\n",
            "        return model.feature_importances_\n",
            "    return np.zeros(len(FEATURES))\n",
            "\n",
            "\n",
            "for type in MODEL_TYPES:\n",
            "    _, axs = plt.subplots(1, len(TARGETS), figsize=(\n",
            "        30, 10), constrained_layout=True)\n",
            "    for i, target in enumerate(TARGETS):\n",
            "        model = MODELS[type][target]\n",
            "        columns = FEATURES\n",
            "        weights = retrieve_weights(model)\n",
            "        axs[i].barh(y=columns, width=weights, edgecolor=\"black\")\n",
            "        axs[i].set_title(\n",
            "            f\"Model: {type} - feature importances for target {target}\")\n",
            "    plt.show()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Analizując ważność parametrów możemy zauważyć, że większość parametrów jest równie ważna, jednakże kilka z nich wyróżnia się na tle pozostałych. W przypadku przewidywania dla tego, czy użytkownik zakupi premium w następnym miesiącu najważniejszym parametrem jest `number_of_advertisements`, czyli liczba wyświetlanych reklam. Tak, więc jeżeli chcemy aby użytkownik jak najszybciej zakupił premium powinniśmy wyświetlać mu jak najwięcej reklam. Natomiast w przypadku przewidywania tego, czy użytkownik kiedykolwiek zakupi premium ważniejsze staje się `average_popularity` oraz `number_of_licked_tracks_listened` co oznacza, że użytkownicy, którzy słuchają popularniejszych utworów oraz słuchają polubionych utworów są bardziej skłonni do zakupu premium. Może oznaczać, to, że w długofalowej perspektywie ważniejsze może być wyświetlanie użytkownikowi utworów, które są popularne oraz utworów, które użytkownik polubił, niż wyświetlanie reklam. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# temp = 10\n",
            "# plots = []\n",
            "# MONTHS = 60\n",
            "# subplots = [plt.subplots(4, MONTHS//4, figsize=(100, 40))\n",
            "#             [1].flatten() for _ in TARGETS]\n",
            "# plot_statistics = []\n",
            "# for year, month in itertools.product(range(2019, 2023), range(1, 13)):\n",
            "#     temp += 1\n",
            "#     if temp % 10 != 0:\n",
            "#         continue\n",
            "#     data_train = data_frame.loc[\n",
            "#         data_frame.apply(lambda x: x.year < year or (\n",
            "#             x.month <= month and x.year == year), axis=1),\n",
            "#         :\n",
            "#     ]\n",
            "#     if len(data_train) == 0:\n",
            "#         continue\n",
            "#     data_test = data_frame.loc[\n",
            "#         data_frame.apply(lambda x: (x.month == month + 1 and x.year == year)\n",
            "#                          or (x.year == year + 1 and x.month == 1), axis=1),\n",
            "#         :\n",
            "#     ]\n",
            "#     x_train, y_train = data_train[FEATURES], data_train[TARGETS]\n",
            "#     x_test, y_test = data_test[FEATURES], data_test[TARGETS]\n",
            "\n",
            "#     plots.append(create_plot_from_model(x_train, y_train, x_test, y_test, [\n",
            "#                  subplot[temp] for subplot in subplots], XGBClassifier, randomized_search_cv.best_params_))\n",
            "\n",
            "# plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# TODO: F1 score, Precision, Recall figures"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Eksperymenty A/B\n",
            "Trenujemy wszystkie modele na danych do 2023, a wyniki dla wszystkich modeli zapisujemy do plików pkl. Uruchamiamy mikroserwis, który wczytuje te modele. Następnie dane użytkowników korzystających w roku 2023 dzielimy na różne rzeczywistości i dla każdej z tych grup wykonujemy predykcję z wykorzystaniem naszego mikroserwisu, a następnie przeprowadzamy porównanie za pomocą testu t-studenta."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_params(model: Model) -> Optional[Dict[str, Any]]:\n",
            "    if isinstance(model, XGBClassifier):\n",
            "        return model.get_params()\n",
            "    return None\n",
            "\n",
            "\n",
            "X_train = pd.DataFrame(\n",
            "    pipeline.fit_transform(TRAIN_DATA[FEATURES]),\n",
            "    columns=FEATURES\n",
            ")\n",
            "Y_train = TRAIN_DATA[TARGETS]\n",
            "for type in MODEL_TYPES:\n",
            "    estimators = {}\n",
            "    for target in TARGETS:\n",
            "        y_train = Y_train[target]\n",
            "        estimators[target] = MODEL_CONSTRUCTORS[type](\n",
            "            X_train, y_train, get_params(MODELS[type][target])\n",
            "        )\n",
            "    model = IUMModel(pipeline, estimators)\n",
            "    with open(f'models/{type}.pkl', 'wb') as f:\n",
            "        pickle.dump(model, f)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "random_ordered_ids = np.random.permutation(TEST_DATA['user_id'].unique())\n",
            "size = len(random_ordered_ids) // len(MODEL_TYPES)\n",
            "\n",
            "REALITIES: Dict[str, pd.DataFrame] = {}\n",
            "\n",
            "for i, type in enumerate(MODEL_TYPES):\n",
            "    ids = random_ordered_ids[i * size:(i + 1) * size]\n",
            "    mask = TEST_DATA['user_id'].isin(ids)\n",
            "    REALITIES[type] = TEST_DATA.loc[mask]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for type in MODEL_TYPES:\n",
            "    display(REALITIES[type].head())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result = {\n",
            "    type: {\n",
            "        target: pd.DataFrame({\n",
            "            \"guess\": [],\n",
            "            \"ground_truth\": [],\n",
            "            \"model\": [],\n",
            "            \"year\": [],\n",
            "            \"month\": [],\n",
            "            \"user_id\": [],\n",
            "        })\n",
            "        for target in TARGETS\n",
            "    }\n",
            "    for type in MODEL_TYPES\n",
            "}\n",
            "\n",
            "for type in MODEL_TYPES:\n",
            "    url = f'http://127.0.0.1:5000/predict/{type}'\n",
            "    for i in range(0, len(TEST_DATA)):\n",
            "        row = TEST_DATA.iloc[i].to_dict()\n",
            "        response = requests.post(url, json=row).json()\n",
            "        for target in TARGETS:\n",
            "            current = pd.DataFrame({\n",
            "                \"guess\": [1 if response[target] else 0],\n",
            "                \"ground_truth\": [row[target]],\n",
            "                \"model\": [type],\n",
            "                \"year\": [row['year']],\n",
            "                \"month\": [row['month']],\n",
            "                \"user_id\": [row['user_id']],\n",
            "            })\n",
            "            result[type][target] = pd.concat(\n",
            "                [result[type][target], current], ignore_index=True\n",
            "            )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for type in MODEL_TYPES:\n",
            "    print(type.upper())\n",
            "    for target in TARGETS:\n",
            "        print(target)\n",
            "        print()\n",
            "        print(result[type][target].guess.value_counts())\n",
            "        print()\n",
            "        print(result[type][target].ground_truth.value_counts())\n",
            "        roc_auc_score_value = roc_auc_score(\n",
            "            result[type][target].ground_truth, result[type][target].guess\n",
            "        )\n",
            "        print()\n",
            "        print('ROC AUC score = ', roc_auc_score_value)\n",
            "        print()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for type in MODEL_TYPES:\n",
            "    for target in TARGETS:\n",
            "        result[type][target].to_csv(f'ab_experiment/{type}-{target}.csv')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "BUCKETS = 12\n",
            "T_ALPHA = 2.074\n",
            "\n",
            "\n",
            "def s_p(sigma_A: float, sigma_B: float) -> float:\n",
            "    return sqrt(\n",
            "        (BUCKETS - 1) * (sigma_A ** 2) + (BUCKETS - 1) * (sigma_B ** 2)\n",
            "        / (BUCKETS + BUCKETS - 2)\n",
            "    )\n",
            "\n",
            "\n",
            "def t(q_A: float, q_B: float, s_p_value: float) -> float:\n",
            "    return (q_A - q_B) / (s_p_value * sqrt(1 / BUCKETS + 1 / BUCKETS))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for type_A, type_B in itertools.product(MODEL_TYPES, MODEL_TYPES):\n",
            "    if type_A == type_B:\n",
            "        continue\n",
            "    print(f'{type_A} vs {type_B}'.upper())\n",
            "    print()\n",
            "    for target in TARGETS:\n",
            "        print(target)\n",
            "        reality_A: pd.DataFrame = result[type_A][target]\n",
            "        reality_B: pd.DataFrame = result[type_B][target]\n",
            "\n",
            "        data = pd.concat([reality_A, reality_B])\n",
            "        random_ordered_ids = np.random.permutation(\n",
            "            data['user_id'].unique()\n",
            "        )\n",
            "        size = len(random_ordered_ids) // BUCKETS\n",
            "\n",
            "        reality_A_score = []\n",
            "        reality_B_score = []\n",
            "\n",
            "        for bucket in range(BUCKETS):\n",
            "            ids = random_ordered_ids[bucket * size:(i + 1) * size]\n",
            "            mask = data['user_id'].isin(ids)\n",
            "            bucket_data = data.loc[mask]\n",
            "            reality_A_data = bucket_data.loc[bucket_data['model'] == type_A]\n",
            "            reality_B_data = bucket_data.loc[bucket_data['model'] == type_B]\n",
            "\n",
            "            reality_A_score.append(\n",
            "                roc_auc_score(\n",
            "                    reality_A_data['ground_truth'],\n",
            "                    reality_A_data['guess'],\n",
            "                )\n",
            "            )\n",
            "            reality_B_score.append(\n",
            "                roc_auc_score(\n",
            "                    reality_B_data['ground_truth'],\n",
            "                    reality_B_data['guess']\n",
            "                )\n",
            "            )\n",
            "\n",
            "        s_p_value = s_p(stdev(reality_A_score), stdev(reality_B_score))\n",
            "        if s_p_value != 0:\n",
            "            t_value = t(mean(reality_A_score), mean(\n",
            "                reality_B_score), s_p_value)\n",
            "        else:\n",
            "            t_value = 0\n",
            "\n",
            "        if t_value > T_ALPHA:\n",
            "            print(f'{type_A} is better than {type_B}')\n",
            "        else:\n",
            "            print(f'We can\\'t say that {type_A} is better than {type_B}')\n",
            "        print()"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.3"
      },
      "orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
