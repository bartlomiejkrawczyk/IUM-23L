{
   "cells": [
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Inżynieria Uczenia Maszynowego\n",
            "\n",
            "Studenci:\n",
            "```\n",
            "Bartłomiej Krawczyk\n",
            "Mateusz Brzozowski\n",
            "```"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Temat\n",
            "\n",
            "> “Jakiś czas temu wprowadziliśmy konta premium, które uwalniają użytkowników od słuchania reklam. Nie są one jednak jeszcze zbyt popularne – czy możemy się dowiedzieć, które osoby są bardziej skłonne do zakupu takiego konta?”"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import itertools\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import pickle\n",
            "import requests\n",
            "\n",
            "from math import sqrt\n",
            "from matplotlib import pyplot as plt\n",
            "from scipy.stats import uniform\n",
            "from sklearn.impute import SimpleImputer\n",
            "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
            "from sklearn.pipeline import Pipeline\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from typing import Any, Dict, Optional\n",
            "from xgboost import XGBClassifier\n",
            "\n",
            "from microservice import IUMModel\n",
            "from utility import (\n",
            "    plot_matrix,\n",
            "    construct_dummy,\n",
            "    construct_logistic_reggression,\n",
            "    plot_confusion_matrix,\n",
            "    plot_confusion_matrix_ab_experiment,\n",
            "    plot_feature_importances,\n",
            "    get_params,\n",
            "    compare_models,\n",
            "    Model,\n",
            "    FEATURES,\n",
            "    TARGETS,\n",
            "    TARGET_AND_FEATURES,\n",
            "    DUMMY,\n",
            "    LOGISTIC_REG,\n",
            "    XGB,\n",
            "    XGB_BEST_ESTIMATOR,\n",
            "    MODEL_TYPES,\n",
            "    SERVICE_PREDICTION_MODEL_INIT,\n",
            "    AB_RESULT\n",
            ")"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Cechy i funkcje celu\n",
            "\n",
            "Do trenowania naszych modeli przygotowaliśmy następujące cechy wygenerowane na podstawie dostarczonych danych:\n",
            "- `number_of_advertisements`, ilość odtworzonych reklam w danym miesiącu\n",
            "- `number_of_tracks`, ilość przesłuchanych utworów w danym miesiącu\n",
            "- `number_of_skips`, ilość pominiętych utworów w danym miesiącu\n",
            "- `number_of_likes`, liczba danych lików w danym miesiącu\n",
            "- `number_of_liked_tracks_listened`, liczba przesłuchanych utworów w danym miesiącu, które w momencie odtworzenia były polubione\n",
            "- `number_of_tracks_in_favourite_genre`, liczba przesłuchanych utworów z ulubionego gatunku w danym miesiącu\n",
            "- `total_number_of_favourite_genres_listened`, liczba przesłuchanych gatunków w danym miesiącu należących do ulubionych użytkownika\n",
            "- `average_popularity_in_favourite_genres`, średnia popularność utworów wśród ulubionych gatunków w danym miesiącu\n",
            "- `total_tracks_duration_ms`, całkowity czas przesłuchanych utworów w danym miesiącu\n",
            "- `number_of_different_artists`, ilość przesłuchanych artystów w danym miesiącu\n",
            "- `average_release_date`, średnia data przesłuchanych piosenek w danym miesiącu\n",
            "- `average_duration_ms`, średni czas trwania utworów przesłuchanych w danym miesiącu\n",
            "- `explicit_tracks_ratio`, ułamek \"wulgarnych\" utworów przesłuchanych w danym miesiącu\n",
            "- `average_popularity`, średnia popularność przesłuchanych utworów w danym miesiącu\n",
            "- `average_acousticness`, średnia akustyka przesłuchanych utworów w danym miesiącu\n",
            "- `average_danceability`, średnia taneczność przesłuchanych utworów w danym miesiącu\n",
            "- `average_energy`, średnia moc przesłuchanych utworów w danym miesiącu\n",
            "- `average_instrumentalness`, średnia ilość utworów z wokalem przesłuchanych w danym miesiącu\n",
            "- `average_liveness`, średnie brzmienie utworów na żywo przesłuchanych w danym miesiącu\n",
            "- `average_loudness`, średnia głośność przesłuchanych utworów w danym miesiącu\n",
            "- `average_speechiness`, średnia ilość wokalu w utworach przesłuchanych w danym miesiącu\n",
            "- `average_tempo`, średnia prędkość przesłuchanych utworów w danym miesiącu\n",
            "- `average_valence`, średnia emocjonalność przesłuchanych utworów w danym miesiącu\n",
            "- `average_track_name_length`, średnia długość nazwy utworów przesłuchanych w danym miesiącu\n",
            "- `average_daily_cost`, średni koszt utrzymania przesłuchanych piosenek w danym miesiącu\n",
            "\n",
            "Posiadamy również dwie funkcje celu:\n",
            "- `premium_user_numerical`, która określa czy użytkownik kiedykolwiek kupi premium\n",
            "- `will_buy_premium_next_month_numerical` przedstawiająca to czy użytkownik zakupi premium w przeciągu następnych 30 dni\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "FEATURE_VERSION = 'v1'\n",
            "FEATURE_PATH = f'features/{FEATURE_VERSION}/feature.csv'"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data_frame = pd.read_csv(FEATURE_PATH)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Przykładowe wartości cech oraz funkcji celu"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data_frame.head()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Macierz korelacji cech z wartościami przewidywanymi\n",
            "Sprawdzamy korelację cech, które nie są zbytnio skorelowane między sobą, a za to są skorelowane z targetem."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "correlation_matrix = data_frame.loc[:, TARGET_AND_FEATURES] \\\n",
            "    .corr(method='spearman')\n",
            "\n",
            "plot_matrix(correlation_matrix)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "RANDOM_SEED: int = 48548579\n",
            "np.random.seed(RANDOM_SEED)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Definiujemy pipeline do uzupełnienia danych pustych oraz przeskalowania danych"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pipeline = Pipeline([\n",
            "    (\"simple_imputer\", SimpleImputer()),\n",
            "    (\"standard_scaler\", StandardScaler())\n",
            "])"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Dzielimy dane na dane trenujące oraz testowe do późniejszych eksperymentów A/B"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "TRAINING_UP_TO = 2023\n",
            "TRAIN_DATA: pd.DataFrame = data_frame.loc[data_frame.year < TRAINING_UP_TO, :]\n",
            "TEST_DATA: pd.DataFrame = data_frame.loc[data_frame.year >= TRAINING_UP_TO, :]\n",
            "TEST_SIZE = 0.33"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Pipeline uczony jest na podstawie samych danych testowych\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "X_train_temp, X_test_temp, Y_train, Y_test = train_test_split(\n",
            "    TRAIN_DATA[FEATURES],\n",
            "    TRAIN_DATA[TARGETS],\n",
            "    test_size=TEST_SIZE,\n",
            "    random_state=RANDOM_SEED\n",
            ")\n",
            "X_train_temp: pd.DataFrame\n",
            "X_test_temp: pd.DataFrame\n",
            "Y_train: pd.DataFrame\n",
            "Y_test: pd.DataFrame\n",
            "\n",
            "train_data = pipeline.fit_transform(X_train_temp)\n",
            "test_data = pipeline.transform(X_test_temp)\n",
            "X_train = pd.DataFrame(train_data, columns=FEATURES)\n",
            "X_test = pd.DataFrame(test_data, columns=FEATURES)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Cechy przetworzone przez pipeline"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "X_train.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "Y_train.head()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Modele\n",
            "\n",
            "Do porównywania wybraliśmy cztery modele:\n",
            "\n",
            "- `Dummy` - naiwny model, który zawsze przewiduje najczęściej występującą klasę\n",
            "- `Logistic Regression` - model regresji logistycznej z domyślnymi parametrami\n",
            "- `XGB Classifier` - model XGBoost z domyślnymi parametrami\n",
            "- `XGB Classifier with Randomized Search` - model XGBoost z Randomized Search. Randomized Search to metoda optymalizacji hiperparametrów, która polega na losowym testowaniu zdefiniowane wartości hiperparametrów i ustaleniu ich najlepszej kombinacji. W ten sposób można znaleźć dobre parametry modelu bez konieczności przeszukiwania całej przestrzeni hiperparametrów. Dodatkowo, aby przeciwdziałać niezbalansowanym danym ustawiliśmy parametr `scale_pos_weight` według zaleceń na stosunek liczby negatywnych rekordów (0) do liczby pozytywnych (1)."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def construct_xgb_classifier(\n",
            "    X_train: pd.DataFrame,\n",
            "    y_train: pd.DataFrame,\n",
            "    params: Optional[Dict[str, Any]] = None\n",
            ") -> XGBClassifier:\n",
            "    return XGBClassifier(\n",
            "        random_state=RANDOM_SEED\n",
            "    ).fit(X_train, y_train)\n",
            "\n",
            "\n",
            "def construct_xgb_classifier_with_randomized_search(\n",
            "    X_train: pd.DataFrame,\n",
            "    y_train: pd.DataFrame,\n",
            "    params: Optional[Dict[str, Any]] = None\n",
            ") -> XGBClassifier:\n",
            "    if params:\n",
            "        return XGBClassifier(**params).fit(X_train, y_train)\n",
            "    scale = y_train.value_counts()\n",
            "    model = XGBClassifier(scale_pos_weight=scale[0] / scale[1])\n",
            "\n",
            "    randomized_search_cv = RandomizedSearchCV(\n",
            "        estimator=model,\n",
            "        param_distributions={\n",
            "            # 'max_depth': range(3, 25),\n",
            "            'max_depth': range(3, 10),\n",
            "            'eta': uniform(0, 0.3),\n",
            "            'gamma': uniform(0, 1),\n",
            "            'n_estimators': range(10, 100),\n",
            "        },\n",
            "        # n_iter=120,\n",
            "        n_iter=1,\n",
            "        scoring='roc_auc',\n",
            "        n_jobs=-1,\n",
            "        verbose=3,\n",
            "        random_state=RANDOM_SEED\n",
            "    )\n",
            "    estimator = randomized_search_cv.fit(X_train, y_train)\n",
            "    best_estimator = estimator.best_estimator_\n",
            "    print('Best parameters:', best_estimator.get_params())  # type: ignore\n",
            "    return best_estimator  # type: ignore\n",
            "\n",
            "\n",
            "MODEL_CONSTRUCTORS = {\n",
            "    DUMMY: construct_dummy,\n",
            "    LOGISTIC_REG: construct_logistic_reggression,\n",
            "    XGB: construct_xgb_classifier,\n",
            "    XGB_BEST_ESTIMATOR: construct_xgb_classifier_with_randomized_search\n",
            "}\n",
            "\n",
            "MODELS: Dict[str, Dict[str, Model]] = {\n",
            "    type: {\n",
            "        target: MODEL_CONSTRUCTORS[type](X_train, Y_train[target])\n",
            "        for target in TARGETS\n",
            "    }\n",
            "    for type in MODEL_TYPES\n",
            "}"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Ocena modeli\n",
            "\n",
            "Posiadamy niezbalansowane dane, dlatego też do oceny modeli wykorzystaliśmy metrykę `ROC-AUC`, która jest miarą jakości klasyfikatora binarnego. \n",
            "\n",
            "`ROC-AUC` mierzy zdolność modelu do rozróżnienia między dwiema klasami poprzez obliczenie powierzchni pod krzywą ROC. Krzywa ROC przedstawia zależność między wskaźnikiem True Positive Rate = TP / ( TP + FN) (czułość) a False Positive Rate = FP / (FP + TN) (specyficzność). Wyższa wartość ROC-AUC oznacza lepszą zdolność modelu do rozróżniania klas.\n",
            "\n",
            "Nie wykorzystaliśmy metryki `accuracy`, ponieważ w przypadku niezbilansowanych danych, może ona być myląca. Przykładowo, jeśli mamy 1000 obserwacji, z czego 900 należy do klasy 0, a 100 do klasy 1, to model, który zawsze zwraca 0, będzie miał accuracy 90%."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for type in MODEL_TYPES:\n",
            "    print(type.upper())\n",
            "    models = MODELS[type]\n",
            "    plot_confusion_matrix(models, X_test, Y_test)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Analizując wyniki możemy zauważyć, że dla przewidywania `premium_user_numerical` (czy użytkownik kiedykolwiek zakupi premium) najgorzej poradził sobie model naiwny `Dummy`, który każdemu przypisuje klasę większościową. Nieznacznie lepsze wyniki na podobnym poziomie, osiągnęły modele `Logistic Regression` oraz `XGB Classifier`. Najlepsze wyniki osiągnął model `XGB Classifier with Randomized Search`, dzięki optymalizacji hiperparametrów. W przypadku `will_buy_premium_next_month_numerical` (czy użytkownik kupi premium w przeciągu miesiąca) modele `Dummy` oraz `Logistic Regression` w każdym przypadku przewidywały klasę większościową. Model `XGB Classifier` był nieznacznie lepszy. Jedynie model `XGB Classifier with Randomized Search` osiągnął lepszy wynik 0.69, jednak kosztem przypisania większej ilości błędnych predykcji w przypadku klasy większościowej."
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Istotność parametrów"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for type in MODEL_TYPES:\n",
            "    models = MODELS[type]\n",
            "    plot_feature_importances(models)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Analizując ważność parametrów możemy zauważyć, że większość parametrów jest brana pod uwagę przez modele, jednakże kilka z nich wyróżnia się na tle pozostałych. W przypadku przewidywania dla tego, czy użytkownik zakupi premium w przeciągu miesiąca najważniejszym parametrem jest `number_of_advertisements`, czyli liczba wyświetlanych reklam. Dodatkowo ostatni wytrenowany model uwzględnia jeszcze `average_release_date` oraz `average_daily_cost`. Możemy z tego wnioskować, że aby użytkownik jak najszybciej zakupił premium powinniśmy manipulować ilością wyświetlanych mu reklam. Natomiast w przypadku przewidywania tego, czy użytkownik kiedykolwiek zakupi premium ważniejsze okazuje się `average_popularity` oraz `number_of_licked_tracks_listened` co oznacza, że użytkownicy, którzy słuchają popularniejszych utworów oraz słuchają polubionych utworów są bardziej skłonni do zakupu premium. Może oznaczać, to, że w długofalowej perspektywie ważniejsze może być proponowanie użytkownikowi utworów, które są popularne oraz utworów, które użytkownik polubił, niż wyświetlanie reklam. "
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Eksperymenty A/B\n",
            "Trenujemy wszystkie modele na danych do 2023, a wyniki dla wszystkich modeli zapisujemy do plików pkl. Uruchamiamy mikroserwis, który wczytuje te modele. Następnie dane użytkowników korzystających w roku 2023 dzielimy na różne rzeczywistości i dla każdej z tych grup wykonujemy predykcję z wykorzystaniem naszego mikroserwisu, a następnie przeprowadzamy porównanie za pomocą testu t-studenta."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "X_train = pd.DataFrame(\n",
            "    pipeline.fit_transform(TRAIN_DATA[FEATURES]),\n",
            "    columns=FEATURES\n",
            ")\n",
            "Y_train = TRAIN_DATA[TARGETS]\n",
            "for type in MODEL_TYPES:\n",
            "    estimators = {}\n",
            "    for target in TARGETS:\n",
            "        y_train = Y_train[target]\n",
            "        estimators[target] = MODEL_CONSTRUCTORS[type](\n",
            "            X_train, y_train, get_params(MODELS[type][target])\n",
            "        )\n",
            "    model = IUMModel(pipeline, estimators)\n",
            "    with open(f'models/{type}.pkl', 'wb') as f:\n",
            "        pickle.dump(model, f)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "URL = 'http://127.0.0.1:5000'"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "requests.post(f'{URL}/init/')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for type in MODEL_TYPES:\n",
            "    url = f'{URL}/ab/'\n",
            "\n",
            "    for i in range(0, len(TEST_DATA)):\n",
            "        row = TEST_DATA.iloc[i].to_dict()\n",
            "        requests.post(url, json=row)  # type: ignore"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "result = SERVICE_PREDICTION_MODEL_INIT\n",
            "\n",
            "for type in MODEL_TYPES:\n",
            "    result[type] = {}\n",
            "    for target in TARGETS:\n",
            "\n",
            "        result[type][target] = pd.merge(\n",
            "            pd.read_csv(\n",
            "                f'ab_experiment/{type}-{target}.csv'\n",
            "            ),\n",
            "            TEST_DATA,\n",
            "            on=['user_id', 'year', 'month']\n",
            "        ).rename(columns={target: 'ground_truth'})[AB_RESULT]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for type in MODEL_TYPES:\n",
            "    for target in TARGETS:\n",
            "        result[type][target].to_csv(f'ab_result/{type}-{target}.csv')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "for type in MODEL_TYPES:\n",
            "    print(type.upper())\n",
            "    plot_confusion_matrix_ab_experiment(result[type])\n",
            "    plt.show()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Ustalamy hipotezę zerową $H_0$ mówiącą, że pierwszy model nie jest lepszy od drugiego oraz hipotezę alternatywną $H_1$ głoszącą, że model pierwszy jest lepszy od modelu drugiego.\n",
            "\n",
            "Na podstawie tabeli rozkładu t-studenta, przyjętego istotności statystycznej jako $0.05$ oraz stopni swobody $12 + 12 - 2 = 22$ ustaliliśmy wartość parametru $t_\\alpha$ jako $2.074$\n",
            "\n",
            "$$\n",
            "t = \\frac{\\overline{q}_A - \\overline{q}_B}{s_p  \\sqrt{\\frac{1}{n_A} + \\frac{1}{n_B}}}\n",
            "$$\n",
            "\n",
            "$$\n",
            "s_p = \\sqrt{\\frac{(n_A - 1) \\sigma^2 + (n_B - 1) \\sigma^2}{n_A + n_B - 2}}\n",
            "$$"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "BUCKETS = 12\n",
            "T_ALPHA = 2.074\n",
            "\n",
            "\n",
            "def s_p(sigma_A: float, sigma_B: float) -> float:\n",
            "    return sqrt(\n",
            "        (BUCKETS - 1) * (sigma_A ** 2) + (BUCKETS - 1) * (sigma_B ** 2)\n",
            "        / (BUCKETS + BUCKETS - 2)\n",
            "    )\n",
            "\n",
            "\n",
            "def t(q_A: float, q_B: float, s_p_value: float) -> float:\n",
            "    return (q_A - q_B) / (s_p_value * sqrt(1 / BUCKETS + 1 / BUCKETS))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "np.random.seed(RANDOM_SEED)\n",
            "\n",
            "for type_A, type_B in itertools.product(MODEL_TYPES, MODEL_TYPES):\n",
            "    if type_A == type_B:\n",
            "        continue\n",
            "    print(f'{type_A} vs {type_B}'.upper())\n",
            "    print()\n",
            "    compare_models(result, type_A, type_B, BUCKETS, T_ALPHA, s_p, t) \n",
            "    "
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Z przeprowadzonego eksperymentu wynika, że model `XGB Classifier with Randomized Search` jest lepszy od reszty modeli w przewidywaniu wartości `will_buy_premium_next_month_numerical` oraz jest lepszy od modelu naiwnego `Dummy` w przypadku przewidywania wartości `premium_user_numerical`."
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Analiza kryterium sukcesu\n",
            "\n",
            "W pierwszym etapie przyjęte kryterium sukcesu oparte na metryce accuracy okazało się niewłaściwe, ponieważ miało miejsce silne niezbalansowanie danych, a metryka accuracy nie skupiała się na przewidywaniu osób, które kupią subskrypcję premium, lecz raczej na poprawności ogólnej predykcji. Model dummy, który przypisywał większościową grupę, osiągał zatem wysoką dokładność w tym kontekście.\n",
            "\n",
            "Naszym nowym kryterium sukcesu jest metryka ROC AUC (Receiver Operating Characteristic Area Under Curve), która skupia się na zdolności modelu do rozróżniania pomiędzy klasami, a w tym przypadku na zdolności do przewidywania użytkowników, którzy zakupią subskrypcję premium. Metryka ta preferuje identyfikację użytkowników premium, a nie dostosowuje się jedynie do grupy większościowej. Dla nas istotniejsza jest informacja, czy ktoś kupi subskrypcję, a nie tylko informacja, że nikt nie kupił.\n",
            "\n",
            "W związku z tym, ustaliliśmy nową wartość progową dla oceny sukcesu, która wynosi #TODO. Oznacza to, że projekt zostaje uznany za zaliczony, jeśli osiągnie wynik powyżej tego progu. Ta wartość progowa jest odpowiednia dla metryki ROC AUC i bardziej adekwatna do naszych potrzeb, uwzględniając istotność predykcji zakupu subskrypcji premium.\n",
            "\n",
            "Wprowadzenie nowego kryterium sukcesu pozwoliło nam lepiej ocenić projekt i skupić się na rzeczywistej wartości predykcji w kontekście naszych celów."
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.6"
      },
      "orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
