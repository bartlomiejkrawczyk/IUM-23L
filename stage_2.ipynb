{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import itertools\n",
            "import numpy as np\n",
            "import numpy.typing as npt\n",
            "import pandas as pd\n",
            "import pickle as pkl\n",
            "import requests\n",
            "import seaborn as sns\n",
            "import os\n",
            "\n",
            "from matplotlib import pyplot as plt\n",
            "from scipy.stats import uniform\n",
            "from sklearn.impute import SimpleImputer\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.dummy import DummyClassifier\n",
            "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
            "from sklearn.metrics import (\n",
            "    confusion_matrix,\n",
            "    f1_score,\n",
            "    precision_score,\n",
            "    recall_score\n",
            ")\n",
            "from sklearn.pipeline import Pipeline\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from typing import Any, Dict, List, Tuple\n",
            "from xgboost import XGBClassifier\n",
            "\n",
            "from microservice import IUMModel\n",
            "from utility import (\n",
            "    load_data,\n",
            "    get_buckets_indices,\n",
            "    get_preds_thr,\n",
            "    get_most_optimal_thr,\n",
            "    get_s_p,\n",
            "    get_t,\n",
            "    get_xgb_logreg_f1_scores,\n",
            "    is_xgb_better,\n",
            "    BUCKETS_CNT,\n",
            "    T_ALPHA,\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "FEATURE_VERSION = 'v1'\n",
            "FEATURE_PATH = f\"features/{FEATURE_VERSION}/feature.csv\"\n",
            "\n",
            "FEATURES = [\n",
            "    'number_of_advertisements',\n",
            "    'number_of_tracks',\n",
            "    'number_of_skips',\n",
            "    'number_of_likes',\n",
            "    'number_of_liked_tracks_listened',\n",
            "    'number_of_tracks_in_favourite_genre',\n",
            "    'total_number_of_favourite_genres_listened',\n",
            "    'average_popularity_in_favourite_genres',\n",
            "    'total_tracks_duration_ms',\n",
            "    'number_of_different_artists',\n",
            "    'average_release_date',\n",
            "    'average_duration_ms',\n",
            "    'explicit_tracks_ratio',\n",
            "    'average_popularity',\n",
            "    'average_acousticness',\n",
            "    'average_danceability',\n",
            "    'average_energy',\n",
            "    'average_instrumentalness',\n",
            "    'average_liveness',\n",
            "    'average_loudness',\n",
            "    'average_speechiness',\n",
            "    'average_tempo',\n",
            "    'average_valence',\n",
            "    'average_track_name_length',\n",
            "    'average_daily_cost'\n",
            "]\n",
            "\n",
            "TARGETS = [\n",
            "    \"premium_user_numerical\",\n",
            "    \"will_buy_premium_next_month_numerical\"\n",
            "]\n",
            "\n",
            "TARGET_AND_FEATURES = TARGETS + FEATURES\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data_frame = pd.read_csv(FEATURE_PATH)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data_frame.head()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "correlation_matrix = data_frame.loc[:, TARGET_AND_FEATURES] \\\n",
            "    .corr(method='spearman')\n",
            "\n",
            "plt.figure(figsize=(16, 16))\n",
            "\n",
            "sns.heatmap(\n",
            "    correlation_matrix,\n",
            "    xticklabels=correlation_matrix.columns,  # type: ignore\n",
            "    yticklabels=correlation_matrix.columns,  # type: ignore\n",
            "    annot=True,\n",
            "    annot_kws={\"fontsize\": 7},\n",
            "    fmt=\".0%\",\n",
            "    vmin=-1,\n",
            "    vmax=1,\n",
            ")\n",
            "\n",
            "plt.show()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pipeline = Pipeline([\n",
            "    (\"simple_imputer\", SimpleImputer()),\n",
            "    (\"standard_scaler\", StandardScaler())\n",
            "])\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "TRAINING_UP_TO = 2023\n",
            "TRAIN_DATA: pd.DataFrame = data_frame.loc[data_frame.year < TRAINING_UP_TO, :]\n",
            "TEST_DATA: pd.DataFrame = data_frame.loc[data_frame.year >= TRAINING_UP_TO, :]\n",
            "TEST_SIZE = 0.33\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "TODO: pipeline dobieramy na podstawie samych danych testowych\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "X_train_temp, X_test_temp, Y_train, Y_test = train_test_split(\n",
            "    TRAIN_DATA[FEATURES],\n",
            "    TRAIN_DATA[TARGETS],\n",
            "    test_size=TEST_SIZE\n",
            ")\n",
            "X_train_temp: pd.DataFrame\n",
            "X_test_temp: pd.DataFrame\n",
            "Y_train: pd.DataFrame\n",
            "Y_test: pd.DataFrame\n",
            "\n",
            "train_data = pipeline.fit_transform(X_train_temp)\n",
            "test_data = pipeline.transform(X_test_temp)\n",
            "X_train = pd.DataFrame(train_data, columns=FEATURES)\n",
            "X_test = pd.DataFrame(test_data, columns=FEATURES)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "X_train.head()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "Y_train.head()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# MODELS: Dict[str, List[Classi]] = dict()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "CANDIDATES = 1\n",
            "\n",
            "model = XGBClassifier()\n",
            "# TODO: update with own parameters\n",
            "randomized_search_cv = RandomizedSearchCV(\n",
            "    estimator=model,\n",
            "    param_distributions={\n",
            "        \"max_depth\": np.arange(3, 30, 1),\n",
            "        \"eta\": uniform(0, 0.1),\n",
            "        \"gamma\": uniform(0, 1),\n",
            "        \"n_estimators\": np.arange(10, 100, 1),\n",
            "    },\n",
            "    n_iter=CANDIDATES,\n",
            "    scoring=\"f1\",\n",
            "    n_jobs=-1,\n",
            "    verbose=3,\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "randomized_search_cv.fit(x_train, y_train[TARGETS])\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "y_pred = randomized_search_cv.predict(x_test)\n",
            "for i, target in enumerate(TARGETS):\n",
            "    matrix_y_true = y_test[target]\n",
            "    matrix_y_pred = y_pred[:, i]\n",
            "    f1_score_value = f1_score(matrix_y_true, matrix_y_pred)\n",
            "    print(f\"F1 score for {target}: {f1_score_value}\")\n",
            "    matrix = confusion_matrix(matrix_y_true, matrix_y_pred)\n",
            "    sns.heatmap(\n",
            "        matrix,\n",
            "        annot=True,\n",
            "        fmt='g',\n",
            "        xticklabels=[\"0\", \"1\"],  # type: ignore\n",
            "        yticklabels=[\"0\", \"1\"]  # type: ignore\n",
            "    )\n",
            "    plt.show()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "_, axs = plt.subplots(1, 2, figsize=(30, 5))\n",
            "axs = axs.flatten()\n",
            "idx = 0\n",
            "for names, values, m_name in zip(\n",
            "    [model.feature_names_in_, model.feature_names_in_],\n",
            "    [model.coef_[0], model.feature_importances_],\n",
            "    [\"LogisticRegression\", \"XGBClassifier\"],\n",
            "):\n",
            "    a = axs[idx]\n",
            "    a.barh(y=names, width=values, edgecolor=\"black\")\n",
            "    a.bar_label(a.containers[0], fmt=\"%.2f\")\n",
            "    a.set_title(f\"{m_name} feature importances\")\n",
            "    idx += 1\n",
            "plt.show()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_class_weights(y: pd.Series, weights: List[float]) -> np.ndarray:\n",
            "    return y.apply(lambda x: weights[x]).values\n",
            "\n",
            "\n",
            "def train_score(\n",
            "    model_class: Any,\n",
            "    xgb_params: Dict[str, Any] = dict(),\n",
            ") -> Tuple[List[float], Any, pd.DataFrame, pd.DataFrame]:\n",
            "    _, axs = plt.subplots(2, 5, figsize=(40, 15))\n",
            "    metrics = {\n",
            "        \"f1_score\": [],\n",
            "        \"precision\": [],\n",
            "        \"recall\": [],\n",
            "    }\n",
            "    axs = axs.flatten()\n",
            "    for month_ in np.arange(10) + 1:\n",
            "        train_data = data_frame.loc[data_frame.month <= month_, :]\n",
            "        test_data = data_frame.loc[data_frame.month == month_ + 1, :]\n",
            "        X_train, y_train = train_data[FEATURES], train_data[TARGETS]\n",
            "        X_test, y_test = test_data[FEATURES], test_data[TARGETS]\n",
            "        X_train = pd.DataFrame(\n",
            "            pipeline.fit_transform(X_train), columns=FEATURES)\n",
            "        X_test = pd.DataFrame(pipeline.transform(X_test), columns=FEATURES)\n",
            "        model = model_class(**xgb_params)\n",
            "        model.fit(X_train, y_train)\n",
            "        y_train_proba = model.predict_proba(X_train)\n",
            "        thr = get_most_optimal_thr(y_train, y_train_proba)\n",
            "        y_pred_proba = model.predict_proba(X_test)\n",
            "        y_pred = get_preds_thr(y_pred_proba, thr)\n",
            "        a = axs[month_ - 1]\n",
            "        sns.heatmap(confusion_matrix(y_test, y_pred),\n",
            "                    annot=True, fmt=\"d\", ax=a)\n",
            "        a.set_title(f\"{month_} test\")\n",
            "        metrics[\"f1_score\"].append(f1_score(y_test, y_pred))\n",
            "        metrics[\"precision\"].append(precision_score(y_test, y_pred))\n",
            "        metrics[\"recall\"].append(recall_score(y_test, y_pred))\n",
            "    return metrics, model, X_test, y_test\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def create_plot_from_model(x_train, y_train, x_test, y_test, subplots, model_constructor, model_params=dict()):\n",
            "    data_train = pipeline.fit_transform(x_train)\n",
            "    data_test = pipeline.transform(x_test)\n",
            "    x_train = pd.DataFrame(data_train, columns=FEATURES)\n",
            "    x_test = pd.DataFrame(data_test, columns=FEATURES)\n",
            "    model = model_constructor(**model_params)\n",
            "    model.fit(x_train, y_train)\n",
            "    y_train_proba = model.predict_proba(x_train)\n",
            "    # TODO: get_most_optimal_threshold\n",
            "    thr = 0.2\n",
            "    y_pred_proba = model.predict_proba(x_test)\n",
            "    plots = []\n",
            "    for i, target in enumerate(TARGETS):\n",
            "        y_pred = pd.Series((y_pred_proba[:, i] > thr).astype(int))\n",
            "        matrix_y_true = y_test[target]\n",
            "        matrix_y_pred = y_pred\n",
            "        f1_score_value = f1_score(matrix_y_true, matrix_y_pred)\n",
            "        print(f\"F1 score for {target}: {f1_score_value}\")\n",
            "        print(f\"Precision: {precision_score(matrix_y_true, y_pred)}\")\n",
            "        print(f\"Recall: {recall_score(matrix_y_true, y_pred)}\")\n",
            "        matrix = confusion_matrix(matrix_y_true, matrix_y_pred)\n",
            "        sns.heatmap(\n",
            "            matrix,\n",
            "            annot=True,\n",
            "            fmt='g',\n",
            "            xticklabels=[\"0\", \"1\"],\n",
            "            yticklabels=[\"0\", \"1\"],\n",
            "            annot_kws={\"fontsize\": 40},\n",
            "            ax=subplots[i]\n",
            "        )\n",
            "    return plots\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "temp = 10\n",
            "plots = []\n",
            "MONTHS = 60\n",
            "subplots = [plt.subplots(4, MONTHS//4, figsize=(100, 40))\n",
            "            [1].flatten() for _ in TARGETS]\n",
            "plot_statistics = []\n",
            "for year, month in itertools.product(range(2019, 2023), range(1, 13)):\n",
            "    temp += 1\n",
            "    if temp % 10 != 0:\n",
            "        continue\n",
            "    data_train = data_frame.loc[\n",
            "        data_frame.apply(lambda x: x.year < year or (\n",
            "            x.month <= month and x.year == year), axis=1),\n",
            "        :\n",
            "    ]\n",
            "    if len(data_train) == 0:\n",
            "        continue\n",
            "    data_test = data_frame.loc[\n",
            "        data_frame.apply(lambda x: (x.month == month + 1 and x.year == year)\n",
            "                         or (x.year == year + 1 and x.month == 1), axis=1),\n",
            "        :\n",
            "    ]\n",
            "    x_train, y_train = data_train[FEATURES], data_train[TARGETS]\n",
            "    x_test, y_test = data_test[FEATURES], data_test[TARGETS]\n",
            "\n",
            "    plots.append(create_plot_from_model(x_train, y_train, x_test, y_test, [\n",
            "                 subplot[temp] for subplot in subplots], XGBClassifier, randomized_search_cv.best_params_))\n",
            "\n",
            "plt.show()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# TODO: F1 score, Precision, Recall figures\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "_, axs = plt.subplots(1, 2, figsize=(30, 5))\n",
            "axs = axs.flatten()\n",
            "idx = 0\n",
            "for names, values, m_name in zip(\n",
            "    [logreg.feature_names_in_, xgb.feature_names_in_],\n",
            "    [logreg.coef_[0], xgb.feature_importances_],\n",
            "    [\"LogisticRegression\", \"XGBClassifier\"],\n",
            "):\n",
            "    a = axs[idx]\n",
            "    a.barh(y=names, width=values, edgecolor=\"black\")\n",
            "    a.bar_label(a.containers[0], fmt=\"%.2f\")\n",
            "    a.set_title(f\"{m_name} feature importances\")\n",
            "    idx += 1\n",
            "plt.show()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def train(\n",
            "    model_class,\n",
            "    model_name: str = \"\",\n",
            "    xgb_params=dict(),\n",
            ") -> None:\n",
            "    X_train, y_train = TRAIN_DATA[FEATURES], TRAIN_DATA[TARGETS]\n",
            "    X_train = pd.DataFrame(pipeline.fit_transform(X_train), columns=FEATURES)\n",
            "    model = model_class(**xgb_params)\n",
            "    model.fit(X_train, y_train)\n",
            "    y_train_proba = model.predict_proba(X_train)\n",
            "    threshold = 0.2\n",
            "    ium_model = IUMModel(pipeline, model, threshold)\n",
            "\n",
            "    with open(f\"models/{model_name}.pkl\", \"wb\") as f:\n",
            "        pkl.dump(ium_model, f)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "train(XGBClassifier, \"xgbclassifier\", randomized_search_cv.best_params_)\n",
            "# train(LogisticRegression, \"logistic_regression\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data_frame = pd.read_csv(FEATURE_PATH)\n",
            "\n",
            "data_frame = data_frame.loc[data_frame.month == 11, :]\n",
            "\n",
            "randomized_indices = np.random.permutation(data_frame.index)\n",
            "A = data_frame.loc[randomized_indices[:int(0.5*len(data_frame))]]\n",
            "B = data_frame.loc[randomized_indices[int(0.5*len(data_frame)):]]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def perform_test(model, data):\n",
            "    # Initiate empty DataFrame\n",
            "    export = {\"user_id\": [], \"guess\": [], \"ground_truth\": [], \"model\": []}\n",
            "    export = pd.DataFrame(export)\n",
            "    # Go over all records\n",
            "    for i in range(0, len(data)):\n",
            "        # Extract one record\n",
            "        row_vals = data.iloc[i]\n",
            "        # Save the users id and the ground truth for future use\n",
            "        user_id = row_vals.user_id\n",
            "        ground_truth = row_vals.is_premium\n",
            "        # Delete user id, month and is premium columns as they are unnecessary\n",
            "        row_vals.drop('user_id')\n",
            "        row_vals.drop('month')\n",
            "        row_vals.drop('is_premium')\n",
            "        # Extract only the values\n",
            "        row_vals = row_vals.values\n",
            "        # Prepare the request\n",
            "        features = ','.join(map(str, row_vals))\n",
            "        request = \"http://127.0.0.1:8000/models/\" + model + \"?features=\" + features\n",
            "        # Get prediction from microservice\n",
            "        guess = requests.get(request).json()\n",
            "        # Append prediction to DataFrame\n",
            "        line = pd.DataFrame({\"user_id\": [user_id], \"guess\": [\n",
            "                            guess[\"prediction\"]], \"ground_truth\": [ground_truth], \"model\": [model]})\n",
            "        export = pd.concat([export, line], ignore_index=True)\n",
            "    return export\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "xgb = perform_test(\"xgbclassifier\", A)\n",
            "logic = perform_test(\"logistic_regression\", B)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "xgb.guess.value_counts()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "xgb.ground_truth.value_counts()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "logic.guess.value_counts()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "logic.ground_truth.value_counts()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "xgb.to_csv(os.path.join(\"results\", \"xgb.csv\"), index=None)\n",
            "logic.to_csv(os.path.join(\"results\", \"logic.csv\"), index=None)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(\"f1\", f1_score(xgb.ground_truth, xgb.guess))\n",
            "print(\"f1\", f1_score(logic.ground_truth, logic.guess))\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "np.random.seed(1234)\n",
            "\n",
            "print(f\"{BUCKETS_CNT=}, {T_ALPHA=}\")\n",
            "data = load_data()\n",
            "buckets_indices = get_buckets_indices(data.user_id.values)\n",
            "xgb_f1_score, logreg_f1_score = get_xgb_logreg_f1_scores(data, buckets_indices)\n",
            "print(f\"{np.mean(xgb_f1_score)=}, {np.mean(logreg_f1_score)=}\")\n",
            "s_p = get_s_p(xgb_f1_score, logreg_f1_score)\n",
            "t = get_t(xgb_f1_score, logreg_f1_score, s_p)\n",
            "print(f\"{s_p=}, {t=}\")\n",
            "if is_xgb_better(t):\n",
            "    print(\"XGBClassifier is better than LogisticRegression\")\n",
            "else:\n",
            "    print(\"We can't say that XGBClassifier is better than LogisticRegression\")\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.6"
      },
      "orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
